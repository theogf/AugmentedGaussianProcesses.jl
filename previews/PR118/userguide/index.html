<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Guide · AugmentedGaussianProcesses</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-129106538-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-129106538-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AugmentedGaussianProcesses</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../background/">Background</a></li><li class="is-active"><a class="tocitem" href>User Guide</a><ul class="internal"><li><a class="tocitem" href="#init"><span>Initialization</span></a></li><li><a class="tocitem" href="#train"><span>Training</span></a></li><li><a class="tocitem" href="#pred"><span>Prediction</span></a></li><li><a class="tocitem" href="#Miscellaneous"><span>Miscellaneous</span></a></li></ul></li><li><a class="tocitem" href="../kernel/">Kernels</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/gpclassification/">Gaussian Process Classification</a></li><li><a class="tocitem" href="../examples/gpevents/">Gaussian Process for Event likelihoods</a></li><li><a class="tocitem" href="../examples/gpregression/">Gaussian Process Regression (for large data)</a></li><li><a class="tocitem" href="../examples/heteroscedastic/">Gaussian Process with Heteroscedastic likelihoods</a></li><li><a class="tocitem" href="../examples/multiclassgp/">Gaussian Process Multi-Class Classification</a></li><li><a class="tocitem" href="../examples/onlinegp/">Online Gaussian Process</a></li><li><a class="tocitem" href="../examples/sampling/">Sampling from a GP</a></li></ul></li><li><a class="tocitem" href="../comparison/">Julia GP Packages</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>User Guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>User Guide</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/userguide.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="User-Guide"><a class="docs-heading-anchor" href="#User-Guide">User Guide</a><a id="User-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#User-Guide" title="Permalink"></a></h1><p>There are 3 main actions needed to train and use the different models:</p><ul><li><a href="#init">Initialization</a></li><li><a href="#train">Training</a></li><li><a href="#pred">Prediction</a></li></ul><h2 id="init"><a class="docs-heading-anchor" href="#init">Initialization</a><a id="init-1"></a><a class="docs-heading-anchor-permalink" href="#init" title="Permalink"></a></h2><h3 id="Possible-models"><a class="docs-heading-anchor" href="#Possible-models">Possible models</a><a id="Possible-models-1"></a><a class="docs-heading-anchor-permalink" href="#Possible-models" title="Permalink"></a></h3><p>There are currently 8 possible Gaussian Process models:</p><h4 id="[GP](@ref)"><a class="docs-heading-anchor" href="#[GP](@ref)"><a href="../api/#AugmentedGaussianProcesses.GP"><code>GP</code></a></a><a id="[GP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[GP](@ref)" title="Permalink"></a></h4><p>GP corresponds to the original GP regression model, it is necessarily with a Gaussian likelihood.</p><pre><code class="language-julia hljs">    GP(X_train, y_train, kernel; kwargs...)</code></pre><h4 id="[VGP](@ref)"><a class="docs-heading-anchor" href="#[VGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.VGP"><code>VGP</code></a></a><a id="[VGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[VGP](@ref)" title="Permalink"></a></h4><p>VGP is a variational GP model: a multivariate Gaussian is approximating the true posterior. There is no inducing points augmentation involved. Therefore it is well suited for small datasets (~10^3 samples).</p><pre><code class="language-julia hljs">    VGP(X_train, y_train, kernel, likelihood, inference; kwargs...)</code></pre><h4 id="[SVGP](@ref)"><a class="docs-heading-anchor" href="#[SVGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.SVGP"><code>SVGP</code></a></a><a id="[SVGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[SVGP](@ref)" title="Permalink"></a></h4><p>SVGP is a variational GP model augmented with inducing points. The optimization is done on those points, allowing for stochastic updates and large scalability. The counterpart can be a slightly lower accuracy and the need to select the number and the location of the inducing points (however this is a problem currently worked on).</p><pre><code class="language-julia hljs">    SVGP(kernel, likelihood, inference, Z; kwargs...)</code></pre><p>Where <code>Z</code> is the position of the inducing points.</p><h4 id="[MCGP](@ref)"><a class="docs-heading-anchor" href="#[MCGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.MCGP"><code>MCGP</code></a></a><a id="[MCGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MCGP](@ref)" title="Permalink"></a></h4><p>MCGP is a GP model where the posterior is represented via a collection of samples.</p><pre><code class="language-julia hljs">   MCGP(X_train, y_train, kernel, likelihood, inference; kwargs...)</code></pre><h4 id="[OnlineSVGP](@ref)"><a class="docs-heading-anchor" href="#[OnlineSVGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.OnlineSVGP"><code>OnlineSVGP</code></a></a><a id="[OnlineSVGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[OnlineSVGP](@ref)" title="Permalink"></a></h4><p>OnlineSVGP is an online variational GP model. It is based on the streaming method of Bui 17&#39;, it supports all likelihoods, even with multiple latent GPs.</p><pre><code class="language-julia hljs">    OnlineSVGP(kernel, likelihood, inference, ind_point_algorithm; kwargs...)</code></pre><h4 id="[MOVGP](@ref)"><a class="docs-heading-anchor" href="#[MOVGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.MOVGP"><code>MOVGP</code></a></a><a id="[MOVGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MOVGP](@ref)" title="Permalink"></a></h4><p>MOVGP is a multi output variational GP model based on the principle <code>f_output[i] = sum(A[i, j] * f_latent[j] for j in 1:n_latent)</code>. The number of latent GP is free.</p><pre><code class="language-julia hljs">    MOVGP(X_train, ys_train, kernel, likelihood/s, inference, n_latent; kwargs...)</code></pre><h4 id="[MOSVGP](@ref)"><a class="docs-heading-anchor" href="#[MOSVGP](@ref)"><a href="../api/#AugmentedGaussianProcesses.MOSVGP"><code>MOSVGP</code></a></a><a id="[MOSVGP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MOSVGP](@ref)" title="Permalink"></a></h4><p>MOSVGP is the same thing as <code>MOVGP</code> but with inducing points: a multi output sparse variational GP model, based on Moreno-Muñoz 18&#39;.</p><pre><code class="language-julia hljs">    MOVGP(kernel, likelihood/s, inference, n_latent, n_inducing_points; kwargs...)</code></pre><h4 id="[VStP](@ref)"><a class="docs-heading-anchor" href="#[VStP](@ref)"><a href="../api/#AugmentedGaussianProcesses.VStP"><code>VStP</code></a></a><a id="[VStP](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[VStP](@ref)" title="Permalink"></a></h4><p>VStP is a variational Student-T model where the prior is a multivariate Student-T distribution with scale <code>K</code>, mean <code>μ₀</code> and degrees of freedom <code>ν</code>. The inference is done automatically by augmenting the prior as a scale mixture of inverse gamma.</p><pre><code class="language-julia hljs">    VStP(X_train, y_train, kernel, likelihood, inference, ν; kwargs...)</code></pre><h3 id="likelihood_user"><a class="docs-heading-anchor" href="#likelihood_user">Likelihood</a><a id="likelihood_user-1"></a><a class="docs-heading-anchor-permalink" href="#likelihood_user" title="Permalink"></a></h3><p><code>GP</code> can only have a Gaussian likelihood, while the other have more choices. Here are the ones currently implemented:</p><h4 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h4><p>For <strong>regression</strong>, four likelihoods are available :</p><ul><li>The classical <a href="../api/#AugmentedGaussianProcesses.GaussianLikelihood"><code>GaussianLikelihood</code></a>, for <a href="https://en.wikipedia.org/wiki/Gaussian_noise"><strong>Gaussian noise</strong></a>.</li><li>The <a href="../api/#AugmentedGaussianProcesses.StudentTLikelihood"><code>StudentTLikelihood</code></a>, assuming noise from a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><strong>Student-T</strong></a> distribution (more robust to ouliers).</li><li>The <a href="../api/#AugmentedGaussianProcesses.LaplaceLikelihood"><code>LaplaceLikelihood</code></a>, with noise from a <a href="https://en.wikipedia.org/wiki/Laplace_distribution"><strong>Laplace</strong></a> distribution.</li><li>The <a href="../api/#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>HeteroscedasticLikelihood</code></a>, (in development) where the noise is a function of the input: <span>$Var(X) = λσ^{-1}(g(X))$</span> where <code>g(X)</code> is an additional Gaussian Process and <code>σ</code> is the logistic function.</li></ul><h4 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h4><p>For <strong>classification</strong> one can select among</p><ul><li>The <a href="../api/#AugmentedGaussianProcesses.LogisticLikelihood"><code>LogisticLikelihood</code></a> : a Bernoulli likelihood with a <a href="https://en.wikipedia.org/wiki/Logistic_function"><strong>logistic link</strong></a>.</li><li>The <a href="../api/#AugmentedGaussianProcesses.BayesianSVM"><code>BayesianSVM</code></a> likelihood based on the <a href="https://en.wikipedia.org/wiki/Support_vector_machine#Bayesian_SVM"><strong>frequentist SVM</strong></a>, equivalent to use a hinge loss.</li></ul><h4 id="Event-Likelihoods"><a class="docs-heading-anchor" href="#Event-Likelihoods">Event Likelihoods</a><a id="Event-Likelihoods-1"></a><a class="docs-heading-anchor-permalink" href="#Event-Likelihoods" title="Permalink"></a></h4><p>For likelihoods such as Poisson or Negative Binomial, we approximate a parameter by <code>σ(f)</code>. Two Likelihoods are implemented :</p><ul><li>The <a href="../api/#GPLikelihoods.PoissonLikelihood"><code>PoissonLikelihood</code></a> : A discrete <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson process</a> (one parameter per point) with the scale parameter defined as <code>λσ(f)</code>.</li><li>The <a href="../api/#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>NegBinomialLikelihood</code></a> : The <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative Binomial likelihood</a> where <code>r</code> is fixed and we define the success probability <code>p</code> as <code>σ(f)</code>.</li></ul><h4 id="Multi-class-classification"><a class="docs-heading-anchor" href="#Multi-class-classification">Multi-class classification</a><a id="Multi-class-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-class-classification" title="Permalink"></a></h4><p>There is two available likelihoods for multi-class classification:</p><ul><li>The <a href="../api/#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>SoftMaxLikelihood</code></a>, the most common approach. However no analytical solving is possible.</li><li>The <a href="../api/#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>LogisticSoftMaxLikelihood</code></a>, a modified softmax where the exponential function is replaced by the logistic function. It allows to get a fully conjugate model, <a href="https://arxiv.org/abs/1905.09670"><strong>Corresponding paper</strong></a>.</li></ul><h3 id="More-options"><a class="docs-heading-anchor" href="#More-options">More options</a><a id="More-options-1"></a><a class="docs-heading-anchor-permalink" href="#More-options" title="Permalink"></a></h3><p>There is the project to get distributions from <code>Distributions.jl</code> to work directly as likelihoods.</p><h3 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h3><p>Inference can be done in various ways.</p><ul><li><a href="../api/#AugmentedGaussianProcesses.AnalyticVI"><code>AnalyticVI</code></a> : <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> with closed-form updates. For non-Gaussian likelihoods, this relies on augmented version of the likelihoods. For using Stochastic Variational Inference, one can use <a href="../api/#AugmentedGaussianProcesses.AnalyticSVI"><code>AnalyticSVI</code></a> with the size of the mini-batch as an argument.</li><li><a href="../api/#AugmentedGaussianProcesses.GibbsSampling"><code>GibbsSampling</code></a> : Gibbs Sampling of the true posterior, this also rely on an augmented version of the likelihoods, this is only valid for the <code>VGP</code> model at the moment.</li></ul><p>The two next methods rely on numerical approximation of an integral and I therefore recommend using the classical <code>Descent</code> approach as it will use anyway the natural gradient updates. <code>ADAM</code> seem to give random results.</p><ul><li><a href="../api/#AugmentedGaussianProcesses.QuadratureVI"><code>QuadratureVI</code></a> : Variational Inference with gradients computed by estimating the expected log-likelihood via quadrature.</li><li><a href="../api/#AugmentedGaussianProcesses.MCIntegrationVI"><code>MCIntegrationVI</code></a> : Variational Inference with gradients computed by estimating the expected log-likelihood via Monte Carlo Integration.</li></ul><p>[WIP] : <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a> will be integrated at some point, although generally the Gibbs sampling is preferable when available.</p><h3 id="compat_table"><a class="docs-heading-anchor" href="#compat_table">Compatibility table</a><a id="compat_table-1"></a><a class="docs-heading-anchor-permalink" href="#compat_table" title="Permalink"></a></h3><p>Not all inference are implemented/valid for all likelihoods, here is the compatibility table between them.</p><table><tr><th style="text-align: right">Likelihood/Inference</th><th style="text-align: center">AnalyticVI</th><th style="text-align: center">GibbsSampling</th><th style="text-align: center">QuadratureVI</th><th style="text-align: center">MCIntegrationVI</th></tr><tr><td style="text-align: right">GaussianLikelihood</td><td style="text-align: center">✔ (Analytic)</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">StudentTLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LaplaceLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">HeteroscedasticLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LogisticLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">BayesianSVM</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LogisticSoftMaxLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">(dev)</td></tr><tr><td style="text-align: right">SoftMaxLikelihood</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td></tr><tr><td style="text-align: right">Poisson</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">NegBinomialLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right"></td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td></tr></table><p>(dev) means that the feature is possible and may be developped and tested but is not available yet. All contributions or requests are very welcome!</p><table><tr><th style="text-align: right">Model/Inference</th><th style="text-align: center">AnalyticVI</th><th style="text-align: center">GibbsSampling</th><th style="text-align: center">QuadratureVI</th><th style="text-align: center">MCIntegrationVI</th></tr><tr><td style="text-align: right">VGP</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td></tr><tr><td style="text-align: right">SVGP</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td></tr><tr><td style="text-align: right">MCGP</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">OnlineSVGP</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">MO(S)VGP</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td></tr><tr><td style="text-align: right">VStP</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td></tr></table><p>Note that for MO(S)VGP you can use a mix of different likelihoods.</p><h3 id="Inducing-Points"><a class="docs-heading-anchor" href="#Inducing-Points">Inducing Points</a><a id="Inducing-Points-1"></a><a class="docs-heading-anchor-permalink" href="#Inducing-Points" title="Permalink"></a></h3><p>Both <a href="../api/#AugmentedGaussianProcesses.SVGP"><code>SVGP</code></a> and <a href="../api/#AugmentedGaussianProcesses.MOSVGP"><code>MOSVGP</code></a> do not take data directly as inputs but inducing points instead. AGP.jl directly reexports the <a href="https://github.com/JuliaGaussianProcesses/InducingPoints.jl">InducingPoints.jl</a> package for you to use. For example to use a k-means approach to select <code>100</code> points on your input data you can use:</p><pre><code class="language-julia hljs">    Z = inducingpoints(KmeanAlg(100), X)</code></pre><p><code>Z</code> will always be an <code>AbstractVector</code> and be directly compatible with <code>SVGP</code> and <code>MOSVGP</code></p><p>For <a href="../api/#AugmentedGaussianProcesses.OnlineSVGP"><code>OnlineSVGP</code></a>, since it cannot be assumed that you have data from the start, only an <a href="https://juliagaussianprocesses.github.io/InducingPoints.jl/dev/#Online-Inducing-Points-Selection">online inducing points selection algorithm</a> can be used. The inducing points locations will be initialized with the first batch of data</p><h3 id="Additional-Parameters"><a class="docs-heading-anchor" href="#Additional-Parameters">Additional Parameters</a><a id="Additional-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Parameters" title="Permalink"></a></h3><h4 id="Hyperparameter-optimization"><a class="docs-heading-anchor" href="#Hyperparameter-optimization">Hyperparameter optimization</a><a id="Hyperparameter-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Hyperparameter-optimization" title="Permalink"></a></h4><p>One can optimize the kernel hyperparameters as well as the inducing points location by maximizing the ELBO. All derivations are already hand-coded (no AD needed). One can select the optimization scheme via :</p><ul><li>The <code>optimiser</code> keyword, can be <code>nothing</code> or <code>false</code> for no optimization or can be an optimiser from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a>.</li><li>The <code>Zoptimiser</code> keyword, similar to <code>optimiser</code> it is used for optimizing the inducing points locations, it is by default set to <code>nothing</code> (no optimization).</li></ul><h4 id="meanprior"><a class="docs-heading-anchor" href="#meanprior">PriorMean</a><a id="meanprior-1"></a><a class="docs-heading-anchor-permalink" href="#meanprior" title="Permalink"></a></h4><p>The <code>mean</code> keyword allows you to add different types of prior means:</p><ul><li><a href="../api/#AugmentedGaussianProcesses.ZeroMean"><code>ZeroMean</code></a>, a constant mean that cannot be optimized.</li><li><a href="../api/#AugmentedGaussianProcesses.ConstantMean"><code>ConstantMean</code></a>, a constant mean that can be optimized.</li><li><a href="../api/#AugmentedGaussianProcesses.EmpiricalMean"><code>EmpiricalMean</code></a>, a vector mean with a different value for each point.</li><li><a href="../api/#AugmentedGaussianProcesses.AffineMean"><code>AffineMean</code></a>, <code>μ₀</code> is given by <code>X*w + b</code>.</li></ul><h2 id="train"><a class="docs-heading-anchor" href="#train">Training</a><a id="train-1"></a><a class="docs-heading-anchor-permalink" href="#train" title="Permalink"></a></h2><h3 id="Offline-models"><a class="docs-heading-anchor" href="#Offline-models">Offline models</a><a id="Offline-models-1"></a><a class="docs-heading-anchor-permalink" href="#Offline-models" title="Permalink"></a></h3><p>Training is straightforward after initializing the <code>model</code> by running :</p><pre><code class="language-julia hljs">model, state = train!(model, X_train, y_train; iterations=100, callback=callbackfunction)</code></pre><p>where the <code>callback</code> option is for running a function at every iteration. <code>callbackfunction</code> should be defined as</p><pre><code class="language-julia hljs">function callbackfunction(model, iter)
    # do things here...
end</code></pre><p>The returned <code>state</code> will contain different variables such as some kernel matrices and local variables. You can reuse this state to save some computations when using prediction functions or computing the <a href="@ref"><code>ELBO</code></a>.</p><p>Note that passing <code>X_train</code> and <code>y_train</code> is optional for <a href="../api/#AugmentedGaussianProcesses.GP"><code>GP</code></a>, <a href="../api/#AugmentedGaussianProcesses.VGP"><code>VGP</code></a> and <a href="../api/#AugmentedGaussianProcesses.MCGP"><code>MCGP</code></a></p><h3 id="Online-models"><a class="docs-heading-anchor" href="#Online-models">Online models</a><a id="Online-models-1"></a><a class="docs-heading-anchor-permalink" href="#Online-models" title="Permalink"></a></h3><p>We recommend looking at <a href="/examples/onlinegp/">the tutorial on online Gaussian processes</a>. One needs to pass a state around, i.e.</p><pre><code class="language-julia hljs">    let state=nothing
        for (X_batch, y_batch) in eachbatch((X_train, y_train))
            model, state = train!(model, X_batch, y_batch, state; iterations=10)
        end
    end</code></pre><h2 id="pred"><a class="docs-heading-anchor" href="#pred">Prediction</a><a id="pred-1"></a><a class="docs-heading-anchor-permalink" href="#pred" title="Permalink"></a></h2><p>Once the model has been trained it is finally possible to compute predictions. There always three possibilities :</p><ul><li><code>predict_f(model, X_test; covf=true, fullcov=false)</code> : Compute the parameters (mean and covariance) of the latent normal distributions of each test points. If <code>covf=false</code> return only the mean, if <code>fullcov=true</code> return a covariance matrix instead of only the diagonal.</li><li><code>predict_y(model, X_test)</code> : Compute the point estimate of the predictive likelihood for regression or the label of the most likely class for classification.</li><li><code>proba_y(model, X_test)</code> : Return the mean with the variance of each point for regression or the predictive likelihood to obtain the class <code>y=1</code> for classification.</li></ul><h2 id="Miscellaneous"><a class="docs-heading-anchor" href="#Miscellaneous">Miscellaneous</a><a id="Miscellaneous-1"></a><a class="docs-heading-anchor-permalink" href="#Miscellaneous" title="Permalink"></a></h2><p>🚧 <strong>In construction – Should be developed in the near future</strong> 🚧</p><p>Saving/Loading models</p><p>Once a model has been trained it is possible to save its state in a file by using  <code>save_trained_model(filename,model)</code>, a partial version of the file will be save in <code>filename</code>.</p><p>It is then possible to reload this file by using <code>load_trained_model(filename)</code>. <strong>!!!However note that it will not be possible to train the model further!!!</strong> This function is only meant to do further predictions.</p><p>🚧 Pre-made callback functions 🚧</p><p>There is one (for now) premade function to return a a MVHistory object and callback function for the training of binary classification problems. The callback will store the ELBO and the variational parameters at every iterations included in iter<em>points If `X</em>test<code>and</code>y_test` are provided it will also store the test accuracy and the mean and median test loglikelihood</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../background/">« Background</a><a class="docs-footer-nextpage" href="../kernel/">Kernels »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.7 on <span class="colophon-date" title="Tuesday 19 July 2022 11:59">Tuesday 19 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
