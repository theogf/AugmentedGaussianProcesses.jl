<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API ¬∑ AugmentedGaussianProcesses</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129106538-2', 'auto');
ga('send', 'pageview');
</script><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><h1>AugmentedGaussianProcesses</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../background/">Background</a></li><li><a class="toctext" href="../userguide/">User Guide</a></li><li><a class="toctext" href="../kernel/">Kernels</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li><a class="toctext" href="../comparison/">Julia GP Packages</a></li><li class="current"><a class="toctext" href>API</a><ul class="internal"><li><a class="toctext" href="#Module-1">Module</a></li><li><a class="toctext" href="#Model-Types-1">Model Types</a></li><li><a class="toctext" href="#Likelihood-Types-1">Likelihood Types</a></li><li><a class="toctext" href="#Inference-Types-1">Inference Types</a></li><li><a class="toctext" href="#Functions-and-methods-1">Functions and methods</a></li><li><a class="toctext" href="#Kernels-1">Kernels</a></li><li><a class="toctext" href="#Kernel-functions-1">Kernel functions</a></li><li><a class="toctext" href="#Prior-Means-1">Prior Means</a></li><li><a class="toctext" href="#Index-1">Index</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>API</a></li></ul><a class="edit-page" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/api.md"><span class="fa">ÔÇõ</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>API</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="API-Library-1" href="#API-Library-1">API Library</a></h1><hr/><ul><li><a href="#API-Library-1">API Library</a></li><ul><li><a href="#Module-1">Module</a></li><li><a href="#Model-Types-1">Model Types</a></li><li><a href="#Likelihood-Types-1">Likelihood Types</a></li><li><a href="#Inference-Types-1">Inference Types</a></li><li><a href="#Functions-and-methods-1">Functions and methods</a></li><li><a href="#Kernels-1">Kernels</a></li><li><a href="#Kernel-functions-1">Kernel functions</a></li><li><a href="#Prior-Means-1">Prior Means</a></li><li><a href="#Index-1">Index</a></li></ul></ul><h2><a class="nav-anchor" id="Module-1" href="#Module-1">Module</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AugmentedGaussianProcesses" href="#AugmentedGaussianProcesses.AugmentedGaussianProcesses"><code>AugmentedGaussianProcesses.AugmentedGaussianProcesses</code></a> ‚Äî <span class="docstring-category">Module</span>.</div><div><div><p>General Framework for the data augmented Gaussian Processes</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/AugmentedGaussianProcesses.jl#L1-L5">source</a></section><h2><a class="nav-anchor" id="Model-Types-1" href="#Model-Types-1">Model Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GP" href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p>Class for Gaussian Processes models</p><pre><code class="language-julia">GP(X::AbstractArray{T}, y::AbstractArray, kernel::Kernel;
    noise::Real=1e-5, opt_noise::Bool=true, verbose::Int=0,
    optimizer::Bool=Adam(Œ±=0.01),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N√óD where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>noise</code> : Initial noise of the model</li><li><code>opt_noise</code> : Flag for optimizing the noise œÉ=Œ£(y-f)^2/N</li><li><code>mean</code> : Option for putting a prior mean</li><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>) or set it to <code>false</code> to keep hyperparameters fixed</li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/models/GP.jl#L1-L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.VGP" href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p>Class for variational Gaussian Processes models (non-sparse)</p><pre><code class="language-julia">VGP(X::AbstractArray{T},y::AbstractVector,
kernel::Kernel,
    likelihood::LikelihoodType,inference::InferenceType;
    verbose::Int=0,optimizer::Union{Bool,Optimizer,Nothing}=Adam(Œ±=0.01),atfrequency::Integer=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N√óD where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, a single kernel from the KernelFunctions.jl package</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Bernoulli (with logistic link), Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user-1"><code>Likelihood Types</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table-1"><code>Compatibility Table</code></a></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>) or set it to <code>false</code> to keep hyperparameters fixed</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/models/VGP.jl#L1-L31">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.SVGP" href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p>Class for sparse variational Gaussian Processes</p><pre><code class="language-julia">SVGP(X::AbstractArray{T1},y::AbstractVector{T2},kernel::Kernel,
    likelihood::LikelihoodType,inference::InferenceType, nInducingPoints::Int;
    verbose::Int=0,optimizer::Union{Optimizer,Nothing,Bool}=Adam(Œ±=0.01),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    Zoptimizer::Union{Optimizer,Nothing,Bool}=false,
    ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N√óD where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Student-T, Laplace, Bernoulli (with logistic link), Bayesian SVM, Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user-1"><code>Likelihood</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table-1"><code>Compatibility table</code></a></li><li><code>nInducingPoints</code> : number of inducing points</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>) or set it to <code>false</code> to keep hyperparameters fixed</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>optimizer</code> : Optimizer for inducing point locations (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>)</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/models/SVGP.jl#L1-L30">source</a></section><h2><a class="nav-anchor" id="Likelihood-Types-1" href="#Likelihood-Types-1">Likelihood Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GaussianLikelihood" href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">GaussianLikelihood(œÉ¬≤::T=1e-3) #œÉ¬≤ is the variance</code></pre><p>Gaussian noise :</p><div>\[    p(y|f) = N(y|f,œÉ¬≤)\]</div><p>There is no augmentation needed for this likelihood which is already conjugate to a Gaussian prior</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/gaussian.jl#L1-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.StudentTLikelihood" href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">StudentTLikelihood(ŒΩ::T,œÉ::Real=one(T))</code></pre><p><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t likelihood</a> for regression:</p><div>\[    p(y|f,ŒΩ,œÉ) = Œì(0.5(ŒΩ+1))/(sqrt(ŒΩœÄ) œÉ Œì(0.5ŒΩ)) * (1+(y-f)^2/(œÉ^2ŒΩ))^(-0.5(ŒΩ+1))\]</div><p><code>ŒΩ</code> is the number of degrees of freedom and <code>œÉ</code> is the variance for local scale of the data.</p><hr/><p>For the analytical solution, it is augmented via:</p><div>\[    p(y|f,œâ) = N(y|f,œÉ^2 œâ)\]</div><p>Where <code>œâ ~ IG(0.5ŒΩ,,0.5ŒΩ)</code> where <code>IG</code> is the inverse gamma distribution See paper <a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf">Robust Gaussian Process Regression with a Student-t Likelihood</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/studentt.jl#L1-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LaplaceLikelihood" href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">LaplaceLikelihood(Œ≤::T=1.0)  #  Laplace likelihood with scale Œ≤</code></pre><p>Laplace likelihood for regression:</p><div>\[1/(2Œ≤) exp(-|y-f|/Œ≤)\]</div><p><strong>see <a href="https://en.wikipedia.org/wiki/Laplace_distribution">wiki page</a></strong></p><p>For the analytical solution, it is augmented via:</p><div>\[p(y|f,œâ) = N(y|f,œâ‚Åª¬π)\]</div><p>where <span>$œâ ~ Exp(œâ | 1/(2 Œ≤^2))$</span>, and <code>Exp</code> is the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a> We use the variational distribution <span>$q(œâ) = GIG(œâ | a,b,p)$</span></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/laplace.jl#L1-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticLikelihood" href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">LogisticLikelihood()</code></pre><p>Bernoulli likelihood with a logistic link for the Bernoulli likelihood</p><div>\[    p(y|f) = \sigma(yf) = \frac{1}{1+\exp(-yf)},\]</div><p>(for more info see : <a href="https://en.wikipedia.org/wiki/Logistic_function">wiki page</a>)</p><hr/><p>For the analytic version the likelihood, it is augmented via:</p><div>\[    p(y|f,œâ) = exp(0.5(yf - (yf)^2 œâ))\]</div><p>where <span>$œâ ~ PG(œâ | 1, 0)$</span>, and <code>PG</code> is the Polya-Gamma distribution See paper : <a href="https://arxiv.org/abs/1802.06383">Efficient Gaussian Process Classification Using Polya-Gamma Data Augmentation</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/logistic.jl#L1-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.HeteroscedasticLikelihood" href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">HeteroscedasticLikelihood(Œª::T=1.0)</code></pre><p>Gaussian with heteroscedastic noise given by another gp:</p><div>\[    p(y|f,g) = N(y|f,(Œª œÉ(g))‚Åª¬π)\]</div><p>Where <code>œÉ</code> is the logistic function</p><p>Augmentation will be described in a future paper</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/heteroscedastic.jl#L1-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.BayesianSVM" href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">BayesianSVM()</code></pre><p>The <a href="https://arxiv.org/abs/1707.05532">Bayesian SVM</a> is a Bayesian interpretation of the classical SVM.</p><div>\[p(y|f) ‚àù exp(2 max(1-yf,0))
````

---
For the analytic version of the likelihood, it is augmented via:\]</div><p>math p(y|f,œâ) = 1/(sqrt(2œÄœâ) exp(-0.5((1+œâ-yf)^2/œâ)) `<span>$where$</span>œâ ‚àº ùüô[0,‚àû)`` has an improper prior (his posterior is however has a valid distribution, a Generalized Inverse Gaussian). For reference <a href="http://ecmlpkdd2017.ijs.si/papers/paperID502.pdf">see this paper</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/bayesiansvm.jl#L1-L17">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.SoftMaxLikelihood" href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">    SoftMaxLikelihood()</code></pre><p>Multiclass likelihood with <a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax transformation</a>:</p><div>\[p(y=i|{f‚Çñ}) = exp(f·µ¢)/ ‚àë‚Çñexp(f‚Çñ)\]</div><p>There is no possible augmentation for this likelihood</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/softmax.jl#L1-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticSoftMaxLikelihood" href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">    LogisticSoftMaxLikelihood()</code></pre><p>The multiclass likelihood with a logistic-softmax mapping: :</p><div>\[p(y=i|{f‚Çñ}‚ÇÅ·¥∑) = œÉ(f·µ¢)/‚àë‚Çñ œÉ(f‚Çñ)\]</div><p>where <code>œÉ</code> is the logistic function. This likelihood has the same properties as <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>. ‚Äì-</p><p>For the analytical version, the likelihood is augmented multiple times. More details can be found in the paper <a href="https://arxiv.org/abs/1905.09670">Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/logisticsoftmax.jl#L1-L15">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.PoissonLikelihood" href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">    Poisson Likelihood(Œª::T=1.0)</code></pre><p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson Likelihood</a> where a Poisson distribution is defined at every point in space (careful, it&#39;s different from continous Poisson processes)</p><div>\[    p(y|f) = Poisson(y|ŒªœÉ(f))\]</div><p>Where <code>œÉ</code> is the logistic function Augmentation details will be released at some point (open an issue if you want to see them)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/poisson.jl#L1-L11">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.NegBinomialLikelihood" href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">    NegBinomialLikelihood(r::Int=10)</code></pre><p><a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative Binomial likelihood</a> with number of failures <code>r</code></p><div>\[    p(y|r,f) = binomial(y+r-1,y) (1-œÉ(f)) ≥œÉ(f) ∏\]</div><p>Where <code>œÉ</code> is the logistic function</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/likelihood/negativebinomial.jl#L1-L12">source</a></section><h2><a class="nav-anchor" id="Inference-Types-1" href="#Inference-Types-1">Inference Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticVI" href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p><strong>AnalyticVI</strong></p><p>Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation) All data is used at each iteration (use AnalyticSVI for Stochastic updates)</p><pre><code class="language-julia">AnalyticVI(;œµ::T=1e-5)</code></pre><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/analyticVI.jl#L1-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticSVI" href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><strong>AnalyticSVI</strong> Stochastic Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation)</p><pre><code class="language-julia">AnalyticSVI(nMinibatch::Integer;œµ::T=1e-5,optimizer::Optimizer=InverseDecay())</code></pre><pre><code class="language-none">- `nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `InverseDecay()` (œÅ=(œÑ+iter)^-Œ∫)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/analyticVI.jl#L41-L54">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GibbsSampling" href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">GibbsSampling(;œµ::T=1e-5,nBurnin::Int=100,samplefrequency::Int=1)</code></pre><p>Draw samples from the true posterior via Gibbs Sampling.</p><p><strong>Keywords arguments</strong>     - <code>œµ::T</code> : convergence criteria     - <code>nBurnin::Int</code> : Number of samples discarded before starting to save samples     - <code>samplefrequency::Int</code> : Frequency of sampling</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/gibbssampling.jl#L1-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureVI" href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p><strong>QuadratureVI</strong></p><p>Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><pre><code class="language-julia">QuadratureVI(œµ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Momentum(Œ∑=0.0001))</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria
- `nGaussHermite::Int` : Number of points for the integral estimation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(Œ∑=0.0001)`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/quadratureVI.jl#L1-L15">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureSVI" href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><strong>QuadratureSVI</strong></p><p>Stochastic Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><pre><code class="language-julia">QuadratureSVI(nMinibatch::Integer;œµ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Adam(Œ±=0.1))</code></pre><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria, which can be user defined
- `nGaussHermite::Int` : Number of points for the integral estimation (for the QuadratureVI)
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(Œ∑=0.001)`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/quadratureVI.jl#L49-L64">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationVI" href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p><code>MCIntegrationVI(;œµ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(Œ±=0.1))</code></p><p>Variational Inference solver by approximating gradients via MC Integration.</p><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/MCVI.jl#L1-L11">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationSVI" href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><code>MCIntegrationSVI(;œµ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(Œ±=0.1))</code></p><p>Stochastic Variational Inference solver by approximating gradients via Monte Carlo integration</p><p><strong>Argument</strong></p><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `œµ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/inference/MCVI.jl#L35-L49">source</a></section><h2><a class="nav-anchor" id="Functions-and-methods-1" href="#Functions-and-methods-1">Functions and methods</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.train!" href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><code>train!(model::AbstractGP;iterations::Integer=100,callback=0,convergence=0)</code></p><p>Function to train the given GP <code>model</code>.</p><p><strong>Keyword Arguments</strong></p><p>there are options to change the number of max iterations,</p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>convergence::Function</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/training.jl#L1-L12">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>predict_f</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_y" href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><code>predict_y(model::AbstractGP,X_test::AbstractMatrix)</code></p><p>Return     - the predictive mean of <code>X_test</code> for regression     - the sign of <code>X_test</code> for classification     - the most likely class for multi-class classification     - the expected number of events for an event likelihood</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/predictions.jl#L100-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.proba_y" href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><p><code>proba_y(model::AbstractGP,X_test::AbstractMatrix)</code></p><p>Return the probability distribution p(y<em>test|model,X</em>test) :</p><pre><code class="language-none">- Tuple of vectors of mean and variance for regression
- Vector of probabilities of y_test = 1 for binary classification
- Dataframe with columns and probability per class for multi-class classification</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/predictions.jl#L127-L135">source</a></section><h2><a class="nav-anchor" id="Kernels-1" href="#Kernels-1">Kernels</a></h2><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>RBFKernel</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MaternKernel</code>. Check Documenter&#39;s build log for details.</p></div></div><h2><a class="nav-anchor" id="Kernel-functions-1" href="#Kernel-functions-1">Kernel functions</a></h2><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>kernelmatrix</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>kernelmatrix!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>getvariance</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>getlengthscales</code>. Check Documenter&#39;s build log for details.</p></div></div><h2><a class="nav-anchor" id="Prior-Means-1" href="#Prior-Means-1">Prior Means</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.ZeroMean" href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p>ZeroMean</p><pre><code class="language-julia">ZeroMean()</code></pre><p>Construct a mean prior set to 0 and cannot be changed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/prior/zeromean.jl#L4-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.ConstantMean" href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p><strong>ConstantMean</strong></p><pre><code class="language-julia">ConstantMean(c::T=1.0;opt::Optimizer=Adam(Œ±=0.01))</code></pre><p>Construct a prior mean with constant <code>c</code> Optionally set an optimizer <code>opt</code> (<code>Adam(Œ±=0.01)</code> by default)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/prior/constantmean.jl#L6-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.EmpiricalMean" href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><p><strong>EmpiricalMean</strong> <code>julia` function EmpiricalMean(c::V=1.0;opt::Optimizer=Adam(Œ±=0.01)) where {V&lt;:AbstractVector{&lt;:Real}}</code> Construct a constant mean with values <code>c</code> Optionally give an optimizer <code>opt</code> (<code>Adam(Œ±=0.01)</code> by default)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/28edc25a1bdb2412cb56f1e91f41cbb71475a1cc/src/prior/empiricalmean.jl#L6-L13">source</a></section><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><li><a href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a></li><li><a href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a></li><li><a href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a></li><li><a href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a></li><li><a href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a></li><li><a href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a></li><li><a href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a></li><li><a href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a></li><li><a href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a></li><li><a href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a></li><li><a href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a></li><li><a href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a></li><li><a href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a></li></ul><footer><hr/><a class="previous" href="../comparison/"><span class="direction">Previous</span><span class="title">Julia GP Packages</span></a></footer></article></body></html>
