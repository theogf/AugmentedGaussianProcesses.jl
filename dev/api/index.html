<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · AugmentedGaussianProcesses</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-129106538-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-129106538-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AugmentedGaussianProcesses</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../background/">Background</a></li><li><a class="tocitem" href="../userguide/">User Guide</a></li><li><a class="tocitem" href="../kernel/">Kernels</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/gpclassification/">Gaussian Process Classification</a></li><li><a class="tocitem" href="../examples/gpevents/">Gaussian Process for Event likelihoods</a></li><li><a class="tocitem" href="../examples/gpregression/">Gaussian Process Regression (for large data)</a></li><li><a class="tocitem" href="../examples/heteroscedastic/">Gaussian Process with Heteroscedastic likelihoods</a></li><li><a class="tocitem" href="../examples/multiclassgp/">Gaussian Process Multi-Class Classification</a></li><li><a class="tocitem" href="../examples/onlinegp/">Online Gaussian Process</a></li><li><a class="tocitem" href="../examples/sampling/">Sampling from a GP</a></li></ul></li><li><a class="tocitem" href="../comparison/">Julia GP Packages</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Module"><span>Module</span></a></li><li><a class="tocitem" href="#Model-Types"><span>Model Types</span></a></li><li><a class="tocitem" href="#Likelihood-Types"><span>Likelihood Types</span></a></li><li><a class="tocitem" href="#Inference-Types"><span>Inference Types</span></a></li><li><a class="tocitem" href="#Functions-and-methods"><span>Functions and methods</span></a></li><li><a class="tocitem" href="#Prior-Means"><span>Prior Means</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Library"><a class="docs-heading-anchor" href="#API-Library">API Library</a><a id="API-Library-1"></a><a class="docs-heading-anchor-permalink" href="#API-Library" title="Permalink"></a></h1><hr/><ul><li><a href="#API-Library">API Library</a></li><ul><li><a href="#Module">Module</a></li><li><a href="#Model-Types">Model Types</a></li><li><a href="#Likelihood-Types">Likelihood Types</a></li><li><a href="#Inference-Types">Inference Types</a></li><li><a href="#Functions-and-methods">Functions and methods</a></li><li><a href="#Prior-Means">Prior Means</a></li><li><a href="#Index">Index</a></li></ul></ul><h2 id="Module"><a class="docs-heading-anchor" href="#Module">Module</a><a id="Module-1"></a><a class="docs-heading-anchor-permalink" href="#Module" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AugmentedGaussianProcesses" href="#AugmentedGaussianProcesses.AugmentedGaussianProcesses"><code>AugmentedGaussianProcesses.AugmentedGaussianProcesses</code></a> — <span class="docstring-category">Module</span></header><section><div><p>General Framework for the data augmented Gaussian Processes</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/AugmentedGaussianProcesses.jl#L1-L5">source</a></section></article><h2 id="Model-Types"><a class="docs-heading-anchor" href="#Model-Types">Model Types</a><a id="Model-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GP" href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GP(args...; kwargs...)</code></pre><p>Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>noise</code> : Variance of the likelihood</li><li><code>opt_noise</code> : Flag for optimizing the variance by using the formul σ=Σ(y-f)^2/N</li><li><code>mean</code> : Option for putting a prior mean</li><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/GP.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.VGP" href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VGP(args...; kwargs...)</code></pre><p>Variational Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractArray</code> : Input features, if <code>X</code> is a matrix the choice of colwise/rowwise is given by the <code>obsdim</code> keyword</li><li><code>y::AbstractVector</code> : Output labels</li><li><code>kernel::Kernel</code> : Covariance function, can be any kernel from KernelFunctions.jl</li><li><code>likelihood</code> : Likelihood of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/VGP.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCGP" href="#AugmentedGaussianProcesses.MCGP"><code>AugmentedGaussianProcesses.MCGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MCGP(args...; kwargs...)</code></pre><p>Monte-Carlo Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractArray</code> : Input features, if <code>X</code> is a matrix the choice of colwise/rowwise is given by the <code>obsdim</code> keyword</li><li><code>y::AbstractVector</code> : Output labels</li><li><code>kernel::Kernel</code> : Covariance function, can be any kernel from KernelFunctions.jl</li><li><code>likelihood</code> : Likelihood of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, at the moment only <a href="#AugmentedGaussianProcesses.GibbsSampling"><code>GibbsSampling</code></a> is available (see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/MCGP.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.SVGP" href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SVGP(args...; kwargs...)</code></pre><p>Sparse Variational Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractArray</code> : Input features, if <code>X</code> is a matrix the choice of colwise/rowwise is given by the <code>obsdim</code> keyword</li><li><code>y::AbstractVector</code> : Output labels</li><li><code>kernel::Kernel</code> : Covariance function, can be any kernel from KernelFunctions.jl</li><li><code>likelihood</code> : Likelihood of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li><li><code>nInducingPoints/Z</code> : number of inducing points, or <code>AbstractVector</code> object</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>Zoptimiser</code> : Optimiser for inducing points locations</li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/SVGP.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.OnlineSVGP" href="#AugmentedGaussianProcesses.OnlineSVGP"><code>AugmentedGaussianProcesses.OnlineSVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OnlineSVGP(args...; kwargs...)</code></pre><p>Online Sparse Variational Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>kernel::Kernel</code> : Covariance function, can be any kernel from KernelFunctions.jl</li><li><code>likelihood</code> : Likelihood of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li><li><code>Zalg</code> : Algorithm selecting how inducing points are selected</li></ul><p><strong>Keywords arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>Zoptimiser</code> : Optimiser for inducing points locations</li><li><code>T::DataType=Float64</code> : Hint for what the type of the data is going to be.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/OnlineSVGP.jl#L12-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MOVGP" href="#AugmentedGaussianProcesses.MOVGP"><code>AugmentedGaussianProcesses.MOVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MOVGP(args...; kwargs...)</code></pre><p>Multi-Output Variational Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractVector</code> : : Input features, if <code>X</code> is a matrix the choice of colwise/rowwise is given by the <code>obsdim</code> keyword</li><li><code>y::AbstractVector{&lt;:AbstractVector}</code> : Output labels, each vector corresponds to one output dimension</li><li><code>kernel::Union{Kernel,AbstractVector{&lt;:Kernel}</code> : covariance function or vector of covariance functions, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood::Union{AbstractLikelihood,Vector{&lt;:Likelihood}</code> : Likelihood or vector of likelihoods of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, for compatibilities see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li><li><code>num_latent::Int</code> : Number of latent GPs</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a> library. Default is <code>ADAM(0.001)</code></li><li><code>Aoptimiser</code> : Optimiser used for the mixing parameters.</li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/MOVGP.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MOSVGP" href="#AugmentedGaussianProcesses.MOSVGP"><code>AugmentedGaussianProcesses.MOSVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MOSVGP(args...; kwargs...)</code></pre><p>Multi-Output Sparse Variational Gaussian Process</p><p><strong>Arguments</strong></p><ul><li><code>kernel::Union{Kernel,AbstractVector{&lt;:Kernel}</code> : covariance function or vector of covariance functions, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihoods::Union{AbstractLikelihood,Vector{&lt;:Likelihood}</code> : Likelihood or vector of likelihoods of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, for compatibilities see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li><li><code>nLatent::Int</code> : Number of latent GPs</li><li><code>nInducingPoints</code> : number of inducing points, or collection of inducing points locations</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a> library. Default is <code>ADAM(0.001)</code></li><li><code>Zoptimiser</code> : Optimiser used for the inducing points locations</li><li><code>Aoptimiser</code> : Optimiser used for the mixing parameters.</li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/MOSVGP.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.VStP" href="#AugmentedGaussianProcesses.VStP"><code>AugmentedGaussianProcesses.VStP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VStP(args...; kwargs...)</code></pre><p>Variational Student-T Process</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractArray</code> : Input features, if <code>X</code> is a matrix the choice of colwise/rowwise is given by the <code>obsdim</code> keyword</li><li><code>y::AbstractVector</code> : Output labels</li><li><code>kernel::Kernel</code> : Covariance function, can be any kernel from KernelFunctions.jl</li><li><code>likelihood</code> : Likelihood of the model. For compatibilities, see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : Inference for the model, see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a>)</li><li><code>ν::Real</code> : Number of degrees of freedom </li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency::Int=1</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean=ZeroMean()</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>obsdim::Int=1</code> : Dimension of the data. 1 : X ∈ DxN, 2: X ∈ NxD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/models/VStP.jl#L1-L20">source</a></section></article><h2 id="Likelihood-Types"><a class="docs-heading-anchor" href="#Likelihood-Types">Likelihood Types</a><a id="Likelihood-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Likelihood-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GaussianLikelihood" href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianLikelihood(σ²::T=1e-3) # σ² is the variance of the noise</code></pre><p>Gaussian noise :</p><p class="math-container">\[    p(y|f) = N(y|f,\sigma^2)\]</p><p>There is no augmentation needed for this likelihood which is already conjugate to a Gaussian prior.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/gaussian.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.StudentTLikelihood" href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StudentTLikelihood(ν::T, σ::Real=one(T))</code></pre><p><strong>Arguments</strong></p><ul><li><code>ν::Real</code> : degrees of freedom of the student-T</li><li><code>σ::Real</code> : standard deviation of the local scale </li></ul><p><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t likelihood</a> for regression:</p><p class="math-container">\[    p(y|f,ν,σ) = \frac{Γ(0.5(ν+1))}{\sqrt(νπ) σ Γ(0.5ν)} (1+\frac{(y-f)^2}{σ^2ν})^{(-0.5(ν+1))},\]</p><p>where <code>ν</code> is the number of degrees of freedom and <code>σ</code> is the standard deviation for local scale of the data.</p><hr/><p>For the augmented analytical solution, it is augmented via:</p><p class="math-container">\[    p(y|f,\omega) = N(y|f,\sigma^2 \omega)\]</p><p>Where <span>$\omega \sim \mathcal{IG}(0.5\nu,0.5\nu)$</span> where <span>$\mathcal{IG}$</span> is the inverse-gamma distribution. See paper <a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf">Robust Gaussian Process Regression with a Student-t Likelihood</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/studentt.jl#L1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LaplaceLikelihood" href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LaplaceLikelihood(β::T=1.0)  #  Laplace likelihood with scale β</code></pre><p>Laplace likelihood for regression:</p><p class="math-container">\[\frac{1}{2\beta} \exp(-\frac{|y-f|}{β})\]</p><p><strong>see <a href="https://en.wikipedia.org/wiki/Laplace_distribution">wiki page</a></strong></p><p>For the analytical solution, it is augmented via:</p><p class="math-container">\[p(y|f,ω) = N(y|f,ω⁻¹)\]</p><p>where <span>$ω \sim \text{Exp}(ω | 1/(2 β^2))$</span>, and <span>$\text{Exp}$</span> is the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a> We use the variational distribution <span>$q(ω) = GIG(ω|a,b,p)$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/laplace.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticLikelihood" href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">LogisticLikelihood() -&gt; BernoulliLikelihood</code></pre><p>Bernoulli likelihood with a logistic link for the Bernoulli likelihood</p><p class="math-container">\[    p(y|f) = \sigma(yf) = \frac{1}{1 + \exp(-yf)},\]</p><p>(for more info see : <a href="https://en.wikipedia.org/wiki/Logistic_function">wiki page</a>)</p><hr/><p>For the analytic version the likelihood, it is augmented via:</p><p class="math-container">\[    p(y|f,ω) = \exp\left(0.5(yf - (yf)^2 \omega)\right)\]</p><p>where <span>$ω \sim \mathcal{PG}(\omega | 1, 0)$</span>, and <span>$\mathcal{PG}$</span> is the Polya-Gamma distribution. See paper : <a href="https://arxiv.org/abs/1802.06383">Efficient Gaussian Process Classification Using Polya-Gamma Data Augmentation</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/logistic.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.HeteroscedasticLikelihood" href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">HeteroscedasticLikelihood(λ::T=1.0)-&gt;HeteroscedasticGaussianLikelihood</code></pre><p><strong>Arguments</strong></p><ul><li><code>λ::Real</code> : The maximum precision possible (this is optimized during training)</li></ul><hr/><p>Gaussian with heteroscedastic noise given by another gp:</p><p class="math-container">\[    p(y|f,g) = \mathcal{N}(y|f,(\lambda \sigma(g))^{-1})\]</p><p>Where <span>$\sigma$</span> is the logistic function</p><p>The augmentation is not trivial and will be described in a future paper</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/heteroscedastic.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.BayesianSVM" href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">BayesianSVM() -&gt; BernoulliLikelihood</code></pre><p>The <a href="https://arxiv.org/abs/1707.05532">Bayesian SVM</a> is a Bayesian interpretation of the classical SVM.</p><p class="math-container">\[p(y|f) \propto \exp(2 \max(1-yf, 0))\]</p><hr/><p>For the analytic version of the likelihood, it is augmented via:</p><p class="math-container">\[p(y|f, ω) = \frac{1}{\sqrt(2\pi\omega)} \exp\left(-\frac{(1+\omega-yf)^2}{2\omega})\right)\]</p><p>where <span>$ω \sim 1[0,\infty)$</span> has an improper prior (his posterior is however has a valid distribution, a Generalized Inverse Gaussian). For reference <a href="http://ecmlpkdd2017.ijs.si/papers/paperID502.pdf">see this paper</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/bayesiansvm.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.SoftMaxLikelihood" href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SoftMaxLikelihood(num_class::Int) -&gt; MultiClassLikelihood</code></pre><p><strong>Arguments</strong></p><ul><li><p><code>num_class::Int</code> : Total number of classes</p><p>SoftMaxLikelihood(labels::AbstractVector) -&gt; MultiClassLikelihood</p></li></ul><p><strong>Arguments</strong></p><ul><li><code>labels::AbstractVector</code> : List of classes labels</li></ul><p>Multiclass likelihood with <a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax transformation</a>:</p><p class="math-container">\[p(y=i|\{f_k\}_{k=1}^K) = \frac{\exp(f_i)}{\sum_{k=1}^K\exp(f_k)}\]</p><p>There is no possible augmentation for this likelihood</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/softmax.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticSoftMaxLikelihood" href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">LogisticSoftMaxLikelihood(num_class::Int) -&gt; MultiClassLikelihood</code></pre><p><strong>Arguments</strong></p><ul><li><p><code>num_class::Int</code> : Total number of classes</p><p>LogisticSoftMaxLikelihood(labels::AbstractVector) -&gt; MultiClassLikelihood</p></li></ul><p><strong>Arguments</strong></p><ul><li><code>labels::AbstractVector</code> : List of classes labels</li></ul><hr/><p>The multiclass likelihood with a logistic-softmax mapping: :</p><p class="math-container">\[p(y=i|\{f_k\}_{1}^{K}) = \frac{\sigma(f_i)}{\sum_{k=1}^k \sigma(f_k)}\]</p><p>where <span>$\sigma$</span> is the logistic function. This likelihood has the same properties as <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>. –-</p><p>For the analytical version, the likelihood is augmented multiple times. More details can be found in the paper <a href="https://arxiv.org/abs/1905.09670">Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/logisticsoftmax.jl#L1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GPLikelihoods.PoissonLikelihood" href="#GPLikelihoods.PoissonLikelihood"><code>GPLikelihoods.PoissonLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PoissonLikelihood(λ::Real)-&gt;PoissonLikelihood</code></pre><p><strong>Arguments</strong></p><ul><li><code>λ::Real</code> : Maximal Poisson rate</li></ul><hr/><p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson Likelihood</a> where a Poisson distribution is defined at every point in space (careful, it&#39;s different from continous Poisson processes).</p><p class="math-container">\[    p(y|f) = \text{Poisson}(y|\lambda \sigma(f))\]</p><p>Where <span>$\sigma$</span> is the logistic function. Augmentation details will be released at some point (open an issue if you want to see them)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/poisson.jl#L1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.NegBinomialLikelihood" href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NegBinomialLikelihood(r::Real)</code></pre><p><strong>Arguments</strong></p><ul><li><code>r::Real</code> number of failures until the experiment is stopped</li></ul><hr/><p><a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative Binomial likelihood</a> with number of failures <code>r</code></p><p class="math-container">\[    p(y|r, f) = {y + r - 1 \choose y} (1 - \sigma(f))^r \sigma(f)^y,\]</p><p>if <span>$r\in \mathbb{N}$</span> or</p><p class="math-container">\[    p(y|r, f) = \frac{\Gamma(y + r)}{\Gamma(y + 1)\Gamma(r)} (1 - \sigma(f))^r \sigma(f)^y,\]</p><p>if <span>$r\in\mathbb{R}$</span>. Where <span>$\sigma$</span> is the logistic function</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/likelihood/negativebinomial.jl#L1-L19">source</a></section></article><h2 id="Inference-Types"><a class="docs-heading-anchor" href="#Inference-Types">Inference Types</a><a id="Inference-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticVI" href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AnalyticVI(;ϵ::T=1e-5)</code></pre><p>Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation) All data is used at each iteration (use <a href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AnalyticSVI</code></a> for updates using minibatches)</p><p><strong>Keywords arguments</strong></p><ul><li><code>ϵ::Real</code> : convergence criteria</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/analyticVI.jl#L16-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticSVI" href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AnalyticSVI(nMinibatch::Int; ϵ::T=1e-5, optimiser=RobbinsMonro())</code></pre><p>Stochastic Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation). See <a href="#AugmentedGaussianProcesses.AnalyticVI"><code>AnalyticVI</code></a> for reference</p><p><strong>Arguments</strong></p><ul><li><code>nMinibatch::Integer</code> : Number of samples per mini-batches</li></ul><p><strong>Keywords arguments</strong></p><ul><li><code>ϵ::T</code> : convergence criteria</li><li><code>optimiser</code> : Optimiser used for the variational updates. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>RobbinsMonro()</code> (ρ=(τ+iter)^-κ)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/analyticVI.jl#L27-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GibbsSampling" href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GibbsSampling(;ϵ::T=1e-5, nBurnin::Int=100, thinning::Int=1)</code></pre><p>Draw samples from the true posterior via Gibbs Sampling.</p><p><strong>Keywords arguments</strong></p><ul><li><code>ϵ::T</code> : convergence criteria</li><li><code>nBurnin::Int</code> : Number of samples discarded before starting to save samples</li><li><code>thinning::Int</code> : Frequency at which samples are saved </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/gibbssampling.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureVI" href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuadratureVI(;ϵ::T=1e-5, nGaussHermite::Integer=20, clipping=Inf, natural::Bool=true, optimiser=Momentum(0.0001))</code></pre><p>Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><p><strong>Keyword arguments</strong></p><ul><li><code>ϵ::T</code> : convergence criteria</li><li><code>nGaussHermite::Int</code> : Number of points for the integral estimation</li><li><code>clipping::Real</code> : Limit the gradients values to avoid overshooting</li><li><code>natural::Bool</code> : Use natural gradients</li><li><code>optimiser</code> : Optimiser used for the variational updates. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>Momentum(0.0001)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/quadratureVI.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureSVI" href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">QuadratureSVI(nMinibatch::Int; ϵ::T=1e-5, nGaussHermite::Int=20, clipping=Inf, natural=true, optimiser=Momentum(0.0001))</code></pre><p>Stochastic Variational Inference solver by approximating gradients via numerical integration via Gaussian Quadrature. See <a href="#AugmentedGaussianProcesses.QuadratureVI"><code>QuadratureVI</code></a> for a more detailed reference.</p><p><strong>Arguments</strong></p><p>-<code>nMinibatch::Integer</code> : Number of samples per mini-batches</p><p><strong>Keyword arguments</strong></p><ul><li><code>ϵ::T</code> : convergence criteria, which can be user defined</li><li><code>nGaussHermite::Int</code> : Number of points for the integral estimation (for the QuadratureVI)</li><li><code>natural::Bool</code> : Use natural gradients</li><li><code>optimiser</code> : Optimiser used for the variational updates. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>Momentum(0.0001)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/quadratureVI.jl#L64-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationVI" href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MCIntegrationVI(;ϵ::T=1e-5, nMC::Integer=1000, clipping::Real=Inf, natural::Bool=true, optimiser=Momentum(0.001))</code></pre><p>Variational Inference solver by approximating gradients via MC Integration. It means the expectation <code>E[log p(y|f)]</code> as well as its gradients is computed by sampling from q(f).</p><p><strong>Keyword arguments</strong></p><ul><li><code>ϵ::Real</code> : convergence criteria, which can be user defined</li><li><code>nMC::Int</code> : Number of samples per data point for the integral evaluation</li><li><code>clipping::Real</code> : Limit the gradients values to avoid overshooting</li><li><code>natural::Bool</code> : Use natural gradients</li><li><code>optimiser</code> : Optimiser used for the variational updates. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>Momentum(0.01)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/MCVI.jl#L36-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationSVI" href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MCIntegrationSVI(batchsize::Int; ϵ::Real=1e-5, nMC::Integer=1000, clipping=Inf, natural=true, optimiser=Momentum(0.0001))</code></pre><p>Stochastic Variational Inference solver by approximating gradients via Monte Carlo integration when using minibatches See <a href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>MCIntegrationVI</code></a> for more explanations.</p><p><strong>Argument</strong></p><p>-<code>batchsize::Integer</code> : Number of samples per mini-batches</p><p><strong>Keyword arguments</strong></p><ul><li><code>ϵ::T</code> : convergence criteria, which can be user defined</li><li><code>nMC::Int</code> : Number of samples per data point for the integral evaluation</li><li><code>clipping::Real</code> : Limit the gradients values to avoid overshooting</li><li><code>natural::Bool</code> : Use natural gradients</li><li><code>optimiser</code> : Optimiser used for the variational updates. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>Momentum()</code> (ρ=(τ+iter)^-κ)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/inference/MCVI.jl#L62-L79">source</a></section></article><h2 id="Functions-and-methods"><a class="docs-heading-anchor" href="#Functions-and-methods">Functions and methods</a><a id="Functions-and-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-and-methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.train!" href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(model::AbstractGPModel; iterations::Integer=100, callback, convergence)</code></pre><p>Function to train the given GP <code>model</code>.</p><p><strong>Arguments</strong></p><ul><li><code>model</code> : AbstractGPModel model with either an <code>Analytic</code>, <code>AnalyticVI</code> or <code>NumericalVI</code> type of inference</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function=nothing</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>convergence::Function=nothing</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/training/training.jl#L1-L12">source</a></section><section><div><pre><code class="nohighlight hljs">train!(model::AbstractGPModel, X::AbstractMatrix, y::AbstractArray; obsdim = 1, iterations::Int=10,callback=nothing,conv=0)
train!(model::AbstractGPModel, X::AbstractVector, y::AbstractArray; iterations::Int=20,callback=nothing,conv=0)</code></pre><p>Function to train the given GP <code>model</code>.</p><p><strong>Keyword Arguments</strong></p><p>there are options to change the number of max iterations,</p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>conv::Function</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/training/onlinetraining.jl#L1-L15">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>sample</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_f" href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict_f(m::AbstractGPModel, X_test, cov::Bool=true, diag::Bool=true)</code></pre><p>Compute the mean of the predicted latent distribution of <code>f</code> on <code>X_test</code> for the variational GP <code>model</code></p><p>Return also the diagonal variance if <code>cov=true</code> and the full covariance if <code>diag=false</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/training/predictions.jl#L132-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_y" href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict_y(model::AbstractGPModel, X_test::AbstractVector)
predict_y(model::AbstractGPModel, X_test::AbstractMatrix; obsdim = 1)</code></pre><p>Return     - the predictive mean of <code>X_test</code> for regression     - 0 or 1 of <code>X_test</code> for classification     - the most likely class for multi-class classification     - the expected number of events for an event likelihood</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/training/predictions.jl#L164-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.proba_y" href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">proba_y(model::AbstractGPModel, X_test::AbstractVector)
proba_y(model::AbstractGPModel, X_test::AbstractMatrix; obsdim = 1)</code></pre><p>Return the probability distribution p(y<em>test|model,X</em>test) :</p><pre><code class="nohighlight hljs">- Tuple of vectors of mean and variance for regression
- Vector of probabilities of y_test = 1 for binary classification
- Dataframe with columns and probability per class for multi-class classification</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/training/predictions.jl#L213-L222">source</a></section></article><h2 id="Prior-Means"><a class="docs-heading-anchor" href="#Prior-Means">Prior Means</a><a id="Prior-Means-1"></a><a class="docs-heading-anchor-permalink" href="#Prior-Means" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.ZeroMean" href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroMean()</code></pre><p>Construct a mean prior set to <code>0</code> and which cannot be updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/mean/zeromean.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.ConstantMean" href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConstantMean(c::Real = 1.0; opt=ADAM(0.01))</code></pre><p><strong>Arguments</strong></p><ul><li><code>c::Real</code> : Constant value</li></ul><p>Construct a prior mean with constant <code>c</code> Optionally set an optimiser <code>opt</code> (<code>ADAM(0.01)</code> by default)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/mean/constantmean.jl#L6-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.EmpiricalMean" href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EmpiricalMean(c::AbstractVector{&lt;:Real}=1.0;opt=ADAM(0.01))</code></pre><p><strong>Arguments</strong></p><ul><li><code>c::AbstractVector</code> : Empirical mean vector</li></ul><p>Construct a empirical mean with values <code>c</code> Optionally give an optimiser <code>opt</code> (<code>ADAM(0.01)</code> by default)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/mean/empiricalmean.jl#L6-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AffineMean" href="#AugmentedGaussianProcesses.AffineMean"><code>AugmentedGaussianProcesses.AffineMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AffineMean(w::Vector, b::Real; opt = ADAM(0.01))
AffineMean(dims::Int; opt=ADAM(0.01))</code></pre><p><strong>Arguments</strong></p><ul><li><code>w::Vector</code> : Weight vector</li><li><code>b::Real</code> : Bias</li><li><code>dims::Int</code> : Number of features per vector</li></ul><p>Construct an affine operation on <code>X</code> : <code>μ₀(X) = X * w + b</code> where <code>w</code> is a vector and <code>b</code> a scalar Optionally give an optimiser <code>opt</code> (<code>Adam(α=0.01)</code> by default)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/eb5eb7f886c209a80e596ea6e0f678dd2e5f0a7b/src/mean/affinemean.jl#L8-L19">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#AugmentedGaussianProcesses.AffineMean"><code>AugmentedGaussianProcesses.AffineMean</code></a></li><li><a href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a></li><li><a href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a></li><li><a href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a></li><li><a href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a></li><li><a href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a></li><li><a href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.MCGP"><code>AugmentedGaussianProcesses.MCGP</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a></li><li><a href="#AugmentedGaussianProcesses.MOSVGP"><code>AugmentedGaussianProcesses.MOSVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.MOVGP"><code>AugmentedGaussianProcesses.MOVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.OnlineSVGP"><code>AugmentedGaussianProcesses.OnlineSVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a></li><li><a href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a></li><li><a href="#AugmentedGaussianProcesses.VStP"><code>AugmentedGaussianProcesses.VStP</code></a></li><li><a href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a></li><li><a href="#GPLikelihoods.PoissonLikelihood"><code>GPLikelihoods.PoissonLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a></li><li><a href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a></li><li><a href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a></li><li><a href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../comparison/">« Julia GP Packages</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.7 on <span class="colophon-date" title="Thursday 7 October 2021 10:17">Thursday 7 October 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
