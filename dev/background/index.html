<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Background · AugmentedGaussianProcesses.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>AugmentedGaussianProcesses.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>Background</a><ul class="internal"><li><a class="toctext" href="#Gaussian-Processes-1">Gaussian Processes</a></li><li><a class="toctext" href="#Augmented-Gaussian-Processes-1">Augmented Gaussian Processes</a></li><li><a class="toctext" href="#Sparse-Gaussian-Processes-1">Sparse Gaussian Processes</a></li></ul></li><li><a class="toctext" href="../userguide/">User Guide</a></li><li><a class="toctext" href="../kernel/">Kernels</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li><a class="toctext" href="../comparison/">Other Julia GP Packages</a></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Background</a></li></ul><a class="edit-page" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/background.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Background</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="The-bits-of-math-and-science-behind-it-1" href="#The-bits-of-math-and-science-behind-it-1">The bits of math and science behind it</a></h1><h2><a class="nav-anchor" id="Gaussian-Processes-1" href="#Gaussian-Processes-1">Gaussian Processes</a></h2><p>To quote <a href="https://en.wikipedia.org/wiki/Gaussian_process">Wikipedia</a> <em>&quot;A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.&quot;</em></p><p>For a detailed understanding of Gaussian processes, check the <a href="http://www.gaussianprocess.org/gpml/">wonderful book of Rasmussen and Williams</a> and for a quick introduction, check <a href="http://mlss2011.comp.nus.edu.sg/uploads/Site/lect1gp.pdf">this tutorial by Zoubin Ghahramani</a></p><p>Gaussian Processes are extremely practical model since they are non-parametric and are Bayesian. However the basic model is limited to regression with Gaussian noise and does not scale very well to large datasets (&gt;1000 samples). The Augmented Gaussian Processes solve both these problems by adding inducing points as well as transforming the likelihood to get efficient variational inference.</p><h2><a class="nav-anchor" id="Augmented-Gaussian-Processes-1" href="#Augmented-Gaussian-Processes-1">Augmented Gaussian Processes</a></h2><p>We are interested in models which consist of a GP prior on a latent function <span>$f\sim \text{GP}(0,k)$</span>, where <span>$k$</span> is the kernel function and the data <span>$y$</span> is connected to <span>$f$</span> via a non-conjugate likelihood <span>$p(y|f)$</span> . We now aim on finding an augmented representation of the model which renders the model conditionally conjugate. Let <span>$\omega$</span> be potential augmentation, then the augmented joint distribution is</p><p><span>$p(y,f,\omega) =p(y|f,\omega)p(\omega)p(f)$</span>.</p><p>The original model can be restored by marginalizing <span>$\omega$</span>, i.e. <span>$p(y,f) =\int p(y,f,\omega)d\omega$</span>.</p><p>The  goal  is  to  find  an  augmentation <span>$\omega$</span>,  such  that  the  augmented  likelihood <span>$p(y|f,\omega$</span>) becomes conjugate to the prior distributions <span>$p(f)$</span> and <span>$p(\omega)$</span> and the expectations of the log complete conditional distributions <span>$\log p(f|\omega,y)$</span> and <span>$\log p(\omega|f,y)$</span> can be computed in closed-form.</p><h4><a class="nav-anchor" id="How-to-find-a-suitable-augmentation?-1" href="#How-to-find-a-suitable-augmentation?-1">How to find a suitable augmentation?</a></h4><p>Many popular likelihood functions can be expressed as ascale mixture of Gaussians</p><div>\[p(y|f) =\int N(y;Bf,\text{diag}(\omega^{−1}))p(\omega)d\omega,\]</div><p>where <span>$B$</span> is a matrix (Palmer et al., 2006).  This representation directly leads to the augmented likelihood <span>$p(y|\omega,f) =N(y;Bf,\text{diag}(\omega^{−1}))$</span> which is conjugate in <span>$f$</span>, i.e. the posterior is Gaussian again.</p><h4><a class="nav-anchor" id="Inference-in-the-augmented-model-1" href="#Inference-in-the-augmented-model-1">Inference in the augmented model</a></h4><p>We n assume that the augmentation, discussed in the previous section, was successful and we obtained an augmented model <span>$p(y,f,\omega) = p(y|f,\omega)p(f)p(\omega)$</span> that is conditionally conjugate. In a conditionally conjugate model variational inference is easy and block coordinate ascent updates can be computed in closed-form. We follow as structured mean-field approach and assume a decoupling between the latent GP <span>$f$</span> and the auxiliary variable <span>$\omega$</span> in the variational distribution <span>$q(f,\omega) = q(f) q(\omega)$</span>.  We alternate between updating <span>$q(f)$</span> and <span>$q(\omega)$</span> by using the typical coordinate ascent (CAVI) updates building on expectations of the log complete conditionals.</p><p>The hyperparameter of the latent GP (e.g. length scale) are learned by optimizing the variational lower bound as function of the hyper parameters. We alternate between updating the variational parameters and the hyperparameters.</p><h2><a class="nav-anchor" id="Sparse-Gaussian-Processes-1" href="#Sparse-Gaussian-Processes-1">Sparse Gaussian Processes</a></h2><p>Direct inference for GPs has a cubic computational complexity <span>$\mathcal{O}(N^3)$</span>. To scale our model to big datasets we approximate the latent GP by a <em>sparse GP</em> building on <em>inducing points</em>. This reduces the complexity to <span>$\mathcal{O}(M^3)$</span>, where <span>$M$</span> is the number of inducing points. Using inducing points allows us to employ stochastic variational inference (SVI) that computes the updates based on mini-batches of the data.</p><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../userguide/"><span class="direction">Next</span><span class="title">User Guide</span></a></footer></article></body></html>
