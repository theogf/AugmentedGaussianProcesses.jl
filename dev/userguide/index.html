<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Guide · AugmentedGaussianProcesses.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>AugmentedGaussianProcesses.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../background/">Background</a></li><li class="current"><a class="toctext" href>User Guide</a><ul class="internal"><li><a class="toctext" href="#init-1">Initialization</a></li><li><a class="toctext" href="#train-1">Training</a></li><li><a class="toctext" href="#pred-1">Prediction</a></li><li><a class="toctext" href="#Miscellaneous-1">Miscellaneous</a></li></ul></li><li><a class="toctext" href="../kernel/">Kernels</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>User Guide</a></li></ul><a class="edit-page" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/userguide.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>User Guide</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="User-Guide-1" href="#User-Guide-1">User Guide</a></h1><p>There are 3 main stages for the GPs:</p><ul><li><a href="#init-1">Initialization</a></li><li><a href="#train-1">Training</a></li><li><a href="#pred-1">Prediction</a></li></ul><h2><a class="nav-anchor" id="init-1" href="#init-1">Initialization</a></h2><h3><a class="nav-anchor" id="Sparse-vs-FullBatch-1" href="#Sparse-vs-FullBatch-1">Sparse vs FullBatch</a></h3><ul><li>A <code>FullBatchModel</code> is a normal GP model, where the variational distribution is optimized over all the training points and no stochastic updates are possible. It is therefore fitted for small datasets (~10^3 samples)</li><li>A <code>SparseModel</code> is a GP model augmented with inducing points. The optimization is done on those points, allowing for stochastic updates and huge scalability. The counterpart can be a slightly lower accuracy and the need to select the number and the location of the inducing points (however this is a problem currently worked on).</li></ul><p>To pick between the two add <code>Batch</code> or <code>Sparse</code> in front of the name of the desired model.</p><p>A better common interface is currently being developed.</p><h3><a class="nav-anchor" id="Regression-1" href="#Regression-1">Regression</a></h3><p>For <strong>regression</strong> one can use the <code>GPRegression</code> or <code>StudentT</code> model. The first one is the vanilla GP with <strong>Gaussian noise</strong> while the second is using the <strong>Student-T</strong> likelihood and is therefore a lot more robust to ouliers.</p><h3><a class="nav-anchor" id="Classification-1" href="#Classification-1">Classification</a></h3><p>For <strong>classification</strong> one can select the <code>XGPC</code> model using the <a href="https://en.wikipedia.org/wiki/Logistic_function"><strong>logistic link</strong></a> or the <code>BSVM</code> model based on the <a href="https://en.wikipedia.org/wiki/Support_vector_machine#Bayesian_SVM"><strong>frequentist SVM</strong></a>.</p><h3><a class="nav-anchor" id="Model-creation-1" href="#Model-creation-1">Model creation</a></h3><p>Creating a model is as simple as doing <code>GPModel(X,y;args...)</code> where <code>args</code> is described in the next section</p><h3><a class="nav-anchor" id="Parameters-of-the-models-1" href="#Parameters-of-the-models-1">Parameters of the models</a></h3><p>All models except for <code>BatchGPRegression</code> use the same set of parameters for initialisation. Default values are showed as well.</p><p>One of the main parameter is the kernel function (or covariance function). This detailed in <a href="https://theogf.github.io/AugmentedGaussianProcesses.jl/latest/Kernels">the kernel section</a>, by default an isotropic RBFKernel with lengthscale 1.0 is used.</p><p>Common parameters :</p><ul><li><code>Autotuning::Bool=true</code> : Are the hyperparameters trained</li><li><code>nEpochs::Integer=100</code> : How many iteration steps are used (can also be set at training time)</li><li>kernel::Kernel : Kernel for the model</li><li><code>noise::Real=1e-3</code> : noise added in the model (is also optimized)</li><li><code>ϵ::Real=1e-5</code> : minimum value for convergence</li><li><code>SmoothingWindow::Integer=5</code> : Window size for averaging convergence in the stochastic case</li><li><code>verbose::Integer=0</code> : How much information is displayed (from 0 to 3)</li></ul><p>Specific to sparse models :</p><ul><li><code>m::Integer=0</code> : Number of inducing points (if not given will be set to a default value depending of the number of points)</li><li><code>Stochastic::Bool=true</code> : Is the method trained via mini batches</li><li><code>batchsize::Integer=-1</code> : number of samples per minibatches (must be set to a correct value or model will fall back for a non stochastic inference)</li><li><code>AdaptiveLearningRate::Bool=true</code> : Is the learning rate adapted via estimation of the gradient variance? see <a href="https://pdfs.semanticscholar.org/9903/e08557f328d58e4ba7fce68faee380d30b12.pdf">&quot;Adaptive Learning Rate for Stochastic Variational inference&quot;</a>, if not use simple exponential decay with parameters <code>κ_s</code> and <code>τ_s</code> seen under <code>(1/(iter+τ_s))^-κ_s</code></li><li><code>κ_s::Real=1.0</code></li><li><code>τ_s::Real=100</code></li><li><code>optimizer::Optimizer=Adam()</code> : Type of optimizer for the inducing point locations</li><li><code>OptimizeIndPoints::Bool=false</code> : Is the location of inducing points optimized, the default is currently to false, as the method is relatively inefficient and does not bring much better performances</li></ul><p>Model specific : <code>StudentT</code> : <code>ν::Real=5</code> : Number of degrees of freedom of the Student-T likelihood</p><h2><a class="nav-anchor" id="train-1" href="#train-1">Training</a></h2><p>Training is straightforward after initializing the model by running :</p><pre><code class="language-julia">model.train(;iterations=100,callback=callbackfunction)</code></pre><p>Where the <code>callback</code> option is for running a function at every iteration. <code>callback function should be defined as</code></p><pre><code class="language-julia">function callbackfunction(model,iter)
    &quot;do things here&quot;...
end</code></pre><h2><a class="nav-anchor" id="pred-1" href="#pred-1">Prediction</a></h2><p>Once the model has been trained it is finally possible to compute predictions. There always three possibilities :</p><ul><li><code>model.fstar(X_test,covf=true)</code> : Compute the parameters (mean and covariance) of the latent normal distributions of each test points. If <code>covf=false</code> return only the mean.</li><li><code>model.predict(X_test)</code> : Compute the point estimate of the predictive likelihood for regression or the label of the most likely class for classification.</li><li><code>model.predictproba(X_test)</code> : Compute the exact predictive likelihood for regression or the predictive likelihood to obtain the class <code>y=1</code> for classification.</li></ul><h2><a class="nav-anchor" id="Miscellaneous-1" href="#Miscellaneous-1">Miscellaneous</a></h2><h3><a class="nav-anchor" id="Saving/Loading-models-1" href="#Saving/Loading-models-1">Saving/Loading models</a></h3><p>Once a model has been trained it is possible to save its state in a file by using  <code>save_trained_model(filename,model)</code>, a partial version of the file will be save in <code>filename</code>.</p><p>It is then possible to reload this file by using <code>load_trained_model(filename)</code>. <strong>!!!However note that it will not be possible to train the model further!!!</strong> This function is only meant to do further predictions.</p><h3><a class="nav-anchor" id="Pre-made-callback-functions-1" href="#Pre-made-callback-functions-1">Pre-made callback functions</a></h3><p>There is one (for now) premade function to return a a MVHistory object and callback function for the training of binary classification problems. The callback will store the ELBO and the variational parameters at every iterations included in iter<em>points If X</em>test and y_test are provided it will also store the test accuracy and the mean and median test loglikelihood</p><footer><hr/><a class="previous" href="../background/"><span class="direction">Previous</span><span class="title">Background</span></a><a class="next" href="../kernel/"><span class="direction">Next</span><span class="title">Kernels</span></a></footer></article></body></html>
